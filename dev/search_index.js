var documenterSearchIndex = {"docs":
[{"location":"api/#API-reference","page":"API reference","title":"API reference","text":"","category":"section"},{"location":"api/#Types","page":"API reference","title":"Types","text":"","category":"section"},{"location":"api/#RadialCalderon.ForwardProblem","page":"API reference","title":"RadialCalderon.ForwardProblem","text":"struct ForwardProblem\n\nContainer for the forward Calderon problem\n\nFields\n\nn::Integer: number of annuli\nradii::Vector: inner radii\n\n\n\n\n\n","category":"type"},{"location":"api/#RadialCalderon.ConvexCalderonProblem","page":"API reference","title":"RadialCalderon.ConvexCalderonProblem","text":"struct ConvexCalderonProblem\n\nContainer for the convex nonlinear SDP problem\n\nFields\n\nc::Vector: weight vector\na::Number: conductivity lower bound\nb::Number: conductivity upper bound\nobs::Vector: observations\nforward::ForwardProblem: forward problem\n\n\n\n\n\n","category":"type"},{"location":"api/#Forward-map","page":"API reference","title":"Forward map","text":"","category":"section"},{"location":"api/#RadialCalderon.forward_map","page":"API reference","title":"RadialCalderon.forward_map","text":"forward_map(problem, j, σ)\n\n\nNeumann-to-Dirichlet forward map\n\n\n\n\n\n","category":"function"},{"location":"api/#Resolution","page":"API reference","title":"Resolution","text":"","category":"section"},{"location":"api/#RadialCalderon.build_c_estimation_problem","page":"API reference","title":"RadialCalderon.build_c_estimation_problem","text":"build_c_estimation_problem(\n    σ,\n    a,\n    b,\n    m,\n    forward;\n    max_last_coord\n)\n\n\nBuild the feasibility problem used to estimate the weight vector\n\n\n\n\n\n","category":"function"},{"location":"api/#RadialCalderon.build_nonlinear_sdp","page":"API reference","title":"RadialCalderon.build_nonlinear_sdp","text":"build_nonlinear_sdp(problem, σ_init; reg_param)\n\n\nBuild the convex nonlinear SDP associated to the input Calderon problem\n\n\n\n\n\n","category":"function"},{"location":"experiments/ill_posedness/#Ill-posedness-of-the-inverse-problem","page":"Ill-posedness","title":"Ill-posedness of the inverse problem","text":"","category":"section"},{"location":"experiments/ill_posedness/","page":"Ill-posedness","title":"Ill-posedness","text":"using RadialCalderon\nusing NonlinearSolve\nusing Random\n\nRandom.seed!(1234);  # fix the random seed for reproducibility\nnothing #hide","category":"page"},{"location":"experiments/ill_posedness/#Mean-error-for-each-annulus","page":"Ill-posedness","title":"Mean error for each annulus","text":"","category":"section"},{"location":"experiments/ill_posedness/","page":"Ill-posedness","title":"Ill-posedness","text":"n = 10\na = 0.5\nb = 1.5\nm = 10  # number of scalar measurements\nnσ_true = 100  # number of unknown conductitivities\nϵ = 1e-15  # tolerance\n\nσ_true_tab = a .+ (b-a) .* rand(n, nσ_true)\n\nσ_hat_tab = zeros(n, nσ_true)\nninit_tab = zeros(Int, nσ_true)\n\nr = reverse([i/n for i=1:n-1])\nforward = ForwardProblem(r)\n\nΛ(σ) = [forward_map(forward, j, σ) for j=1:m]\n\nfor iσ_true=1:nσ_true\n    σ_true = σ_true_tab[:, iσ_true]\n    obs_true = Λ(σ_true)\n\n    converged = false\n    ninit = 0\n    σ_hat = zeros(n)\n\n    while !converged\n        ninit_tab[iσ_true] += 1\n\n        σ_init = a .+ (b-a) .* rand(n)\n\n        f(σ, p) = Λ(σ) .- obs_true\n        problem = NonlinearProblem(f, σ_init)\n\n        try\n            res = NonlinearSolve.solve(problem, TrustRegion(), abstol=ϵ)\n            σ_hat .= res.u\n            converged = maximum(abs.(Λ(σ_hat) .- obs_true)) < ϵ\n        catch error\n            if !(error isa DomainError)\n                rethrow(error)  # rethrow any other error\n            end\n        end\n    end\n\n    σ_hat_tab[:, iσ_true] .= σ_hat\nend","category":"page"},{"location":"experiments/ill_posedness/","page":"Ill-posedness","title":"Ill-posedness","text":"Now, we display the mean error on the i-th annulus as a function of i. The error on the outermost annulus is several orders of magnitude smaller than the error on the innermost annulus.","category":"page"},{"location":"experiments/ill_posedness/","page":"Ill-posedness","title":"Ill-posedness","text":"using Statistics\nusing Plots\nusing LaTeXStrings\n\nmean_err = vcat(mean(abs.(σ_true_tab .- σ_hat_tab), dims=2)...)\n\nplot(mean_err, lc=:red, lw=2, linestyle=:dash, primary=false)\nplot!(\n    mean_err,\n    yscale=:log10,\n    ylim=(1e-14, 1e-7),\n    seriestype=:scatter,\n    ms=5,\n    markerstrokewidth=2,\n    xticks=collect(1:10),\n    xtickfontsize=14,\n    ytickfontsize=14,\n    primary=false,\n    formatter=:latex,\n    mc=:red,\n    linestyle=:dot\n)\nxlabel!(L\"i\", xguidefontsize=18)\nylabel!(L\"\\mathrm{mean}(|\\hat{\\sigma}_i-\\sigma^\\dagger_i|)\", yguidefontsize=18)","category":"page"},{"location":"experiments/ill_posedness/#Mean-error-as-a-function-of-n","page":"Ill-posedness","title":"Mean error as a function of n","text":"","category":"section"},{"location":"experiments/ill_posedness/","page":"Ill-posedness","title":"Ill-posedness","text":"mean_err_tab = zeros(10)\nmean_err_tab[10] = mean(maximum(abs.(σ_hat_tab .- σ_true_tab), dims=1))\n\nfor n=1:9\n    global m = n\n\n    σ_true_tab_n = a .+ (b-a) .* rand(n, nσ_true)\n    σ_hat_tab_n = zeros(n, nσ_true)\n\n    global r = reverse([i/n for i=1:n-1])\n    global forward = ForwardProblem(r)\n\n    Λ(σ) = [forward_map(forward, j, σ) for j=1:m]\n\n    for iσ_true=1:nσ_true\n        σ_true = σ_true_tab_n[:, iσ_true]\n        obs_true = Λ(σ_true)\n\n        converged = false\n        σ_hat = zeros(n)\n\n        while !converged\n            σ_init = a .+ (b-a) .* rand(n)\n\n            f(σ, p) = Λ(σ) .- obs_true\n            problem = NonlinearProblem(f, σ_init)\n\n            try\n                res = NonlinearSolve.solve(problem, TrustRegion(), abstol=1e-15)\n                σ_hat .= res.u\n                converged = maximum(abs.(Λ(σ_hat) .- obs_true)) < 1e-15\n            catch error\n                if !(error isa DomainError)\n                    rethrow(error)  # rethrow any other error\n                end\n            end\n        end\n\n        σ_hat_tab_n[:, iσ_true] .= σ_hat\n    end\n\n    mean_err_tab[n] = mean(maximum(abs.(σ_hat_tab_n .- σ_true_tab_n), dims=1))\nend","category":"page"},{"location":"experiments/ill_posedness/","page":"Ill-posedness","title":"Ill-posedness","text":"Now, we display the mean error as a function of n. The error significantly increases as n increases.","category":"page"},{"location":"experiments/ill_posedness/","page":"Ill-posedness","title":"Ill-posedness","text":"plot(1:10, mean_err_tab, lc=:red, lw=2, linestyle=:dash, primary=false)\nplot!(\n    1:10,\n    mean_err_tab,\n    yscale=:log10,\n    ylim=(1e-17, 1e-7),\n    seriestype=:scatter,\n    ms=5,\n    markerstrokewidth=2,\n    xticks=collect(1:10),\n    yticks=1 ./ (10) .^ reverse(collect(7:17)),\n    xtickfontsize=14,\n    ytickfontsize=14,\n    primary=false,\n    formatter=:latex,\n    mc=:red,\n    linestyle=:dot\n)\nxlabel!(L\"n\", xguidefontsize=18)\nylabel!(L\"\\mathrm{mean}(\\Vert\\hat{\\sigma}-\\sigma^\\dagger\\Vert_{\\infty})\", yguidefontsize=18)","category":"page"},{"location":"experiments/ill_posedness/","page":"Ill-posedness","title":"Ill-posedness","text":"","category":"page"},{"location":"experiments/ill_posedness/","page":"Ill-posedness","title":"Ill-posedness","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/forward/#Evaluating-the-forward-map-and-its-derivatives","page":"Forward map","title":"Evaluating the forward map and its derivatives","text":"","category":"section"},{"location":"examples/forward/","page":"Forward map","title":"Forward map","text":"Here we show how to evaluate the forward map associated to the Calderon problem with radial piecewise constant conductivities. We also show how to evaluate its derivatives via automatic differentiation.","category":"page"},{"location":"examples/forward/","page":"Forward map","title":"Forward map","text":"using RadialCalderon","category":"page"},{"location":"examples/forward/#Setting","page":"Forward map","title":"Setting","text":"","category":"section"},{"location":"examples/forward/","page":"Forward map","title":"Forward map","text":"We define the forward problem. There are three annuli with the inner radii being 0.5 and 0.25.","category":"page"},{"location":"examples/forward/","page":"Forward map","title":"Forward map","text":"n = 3\nr = [0.5, 0.25]\nforward = ForwardProblem(r);\nnothing #hide","category":"page"},{"location":"examples/forward/#Forward-map","page":"Forward map","title":"Forward map","text":"","category":"section"},{"location":"examples/forward/","page":"Forward map","title":"Forward map","text":"We define the (diagonal) Neumann-to-Dirichlet map associated to the boundary data (mathrmcos(jtheta))_1leq jleq m.","category":"page"},{"location":"examples/forward/","page":"Forward map","title":"Forward map","text":"m = 5\nλ(j, σ) = forward_map(forward, j, σ)\nΛ(σ) = [λ(j, σ) for j in 1:m];\nnothing #hide","category":"page"},{"location":"examples/forward/#Derivatives","page":"Forward map","title":"Derivatives","text":"","category":"section"},{"location":"examples/forward/","page":"Forward map","title":"Forward map","text":"The first and second order derivatives of Lambda can be computed via automatic differentiation. The automatic differentiation backend (here ForwardDiff) can be changed easily thanks to the DifferentiationInterface package.","category":"page"},{"location":"examples/forward/","page":"Forward map","title":"Forward map","text":"using DifferentiationInterface\nimport ForwardDiff\n\nbackend = AutoForwardDiff()\n\ndΛ(σ) = jacobian(Λ, backend, σ)  # Jacobian of the forward map\ngrad_λ(j, σ) = gradient(σ -> λ(j, σ), backend, σ)  # gradient of the j-th output\nhess_λ(j, σ) = hessian(σ -> λ(j, σ), backend, σ)  # Hessian of the j-th output\n\ndΛ(ones(n))","category":"page"},{"location":"examples/forward/#Closed-form-expression","page":"Forward map","title":"Closed form expression","text":"","category":"section"},{"location":"examples/forward/","page":"Forward map","title":"Forward map","text":"When n=3 and the conductivity is equal to 1 on the outermost annulus, the forward map has the following closed form expression (Harrach, 2023):","category":"page"},{"location":"examples/forward/","page":"Forward map","title":"Forward map","text":"λ_j(σ_1σ_2)=fracc_j+d_jj(c_j-d_j)","category":"page"},{"location":"examples/forward/","page":"Forward map","title":"Forward map","text":"where","category":"page"},{"location":"examples/forward/","page":"Forward map","title":"Forward map","text":"c_j=(1σ_1+1)(σ_1+σ_2) + (1σ_1-1)(σ_1-σ_2)r_2^2jr_1^2j","category":"page"},{"location":"examples/forward/","page":"Forward map","title":"Forward map","text":"d_j=(1σ_1-1)(σ_1+σ_2)r_1^2j + (1σ_1+1)(σ_1-σ_2)r_2^2j","category":"page"},{"location":"examples/forward/","page":"Forward map","title":"Forward map","text":"We check the consistency of the implemented forward map with this formula.","category":"page"},{"location":"examples/forward/","page":"Forward map","title":"Forward map","text":"σ1 = 1.5\nσ2 = 0.5\nσ = [1.0, σ1, σ2]\n\nc(j) = (1/σ1+1)*(σ1+σ2) + (1/σ1-1)*(σ1-σ2)*r[2]^(2*j)/r[1]^(2*j)\nd(j) = (1/σ1-1)*(σ1+σ2)*r[1]^(2*j) + (1/σ1+1)*(σ1-σ2)*r[2]^(2*j)\n\n@info forward_map(forward, 1, σ) ≈ (c(1)+d(1)) / (c(1)-d(1))\n@info forward_map(forward, 2, σ) ≈ (c(2)+d(2)) / (2*(c(2)-d(2)))\n@info forward_map(forward, 3, σ) ≈ (c(3)+d(3)) / (3*(c(3)-d(3)))","category":"page"},{"location":"examples/forward/","page":"Forward map","title":"Forward map","text":"","category":"page"},{"location":"examples/forward/","page":"Forward map","title":"Forward map","text":"This page was generated using Literate.jl.","category":"page"},{"location":"experiments/ls_convergence/#Convergence-of-least-squares-solvers","page":"Convergence of least squares solvers","title":"Convergence of least squares solvers","text":"","category":"section"},{"location":"experiments/ls_convergence/","page":"Convergence of least squares solvers","title":"Convergence of least squares solvers","text":"In this experiment, we wish to investigate when least squares solvers fail to converge. We found that the best choice of solver is Powell's dog leg method, which is a trust-region method combining the steepest and the Gauss-Newton descent directions. We found that this solver always converges for nleq 9, but sometimes runs for a very large number of iterations. For n=10, it fails to converge on very few instances (less than 1 in 100), and we conjecture that allowing for more iterations would lead to convergence. In any case, we never found an instance where the solver converges to a point which is not an approximate global minimizer of the least squares objective.","category":"page"},{"location":"experiments/ls_convergence/#Setting","page":"Convergence of least squares solvers","title":"Setting","text":"","category":"section"},{"location":"experiments/ls_convergence/","page":"Convergence of least squares solvers","title":"Convergence of least squares solvers","text":"using RadialCalderon\nusing NonlinearSolve\nusing Random\nusing Plots\nusing LaTeXStrings\n\nusing DifferentiationInterface\nimport ForwardDiff\n\nbackend = AutoForwardDiff()  # autodiff backen\n\nRandom.seed!(1234)  # fix the random seed for reproducibility\n\na = 0.5  # conductivity lower bound\nb = 1.5  # conductivity upper bound\n\nnσ_true = 100  # number of true conductivities\noffset = 1e-5  # parameter for the smoothed absolute value\nsolver = TrustRegion();  # trust-region dogleg solver\nnothing #hide","category":"page"},{"location":"experiments/ls_convergence/#Check-convergence-for-n\\leq-9","page":"Convergence of least squares solvers","title":"Check convergence for nleq 9","text":"","category":"section"},{"location":"experiments/ls_convergence/","page":"Convergence of least squares solvers","title":"Convergence of least squares solvers","text":"σ_true_tab_9 = zeros(9, nσ_true)  # save results for $n=9$\nσ_init_tab_9 = zeros(9, nσ_true)\niter_tab_9 = zeros(Int, nσ_true)\n\nfor n=1:9\n    @info \"n=$n\"\n    m = n\n\n    σ_true_tab = a .+ (b-a) .* rand(n, nσ_true)\n    σ_init_tab = a .+ (b-a) .* rand(n, nσ_true)\n\n    σ_hat_tab = zeros(n, nσ_true)\n    iter_tab = zeros(Int, nσ_true)\n    err_tab = zeros(nσ_true)\n    err_tab_measurements = zeros(nσ_true)\n\n    r = reverse([i/n for i=1:n-1])\n    forward = ForwardProblem(r)\n    Λ(σ) = [forward_map(forward, j, σ) for j=1:m]\n\n    for iσ_true=1:nσ_true\n        σ_true = σ_true_tab[:, iσ_true]\n        σ_init = σ_init_tab[:, iσ_true]\n        obs_true = Λ(σ_true)\n\n        f(σ, p) = Λ(sqrt.(σ.^2 .+ offset)) .- obs_true  # re-parameterization to avoid non-positive conductivities\n\n        problem = NonlinearProblem(f, σ_init)\n        res = NonlinearSolve.solve(problem, solver, abstol=1e-15, maxiters=200000)\n        σ_hat = sqrt.((res.u).^2 .+ offset)\n\n        σ_hat_tab[:, iσ_true] .= σ_hat\n        iter_tab[iσ_true] = res.stats.nsteps\n\n        err = abs.(σ_hat .- σ_true) ./ abs.(σ_true)\n        err_tab[iσ_true] = maximum(err)\n        err_tab_measurements[iσ_true] = maximum(abs.(Λ(σ_hat) .- obs_true))\n    end\n\n    @info \"maximum relative error (conductivity): $(maximum(err_tab))\"\n    @info \"maximum error (measurements): $(maximum(err_tab_measurements))\"\n\n    if n == 9\n        σ_true_tab_9 .= σ_true_tab\n        σ_init_tab_9 .= σ_init_tab\n        iter_tab_9 .= iter_tab\n    end\nend","category":"page"},{"location":"experiments/ls_convergence/#Investigate-when-the-number-of-iterations-is-large","page":"Convergence of least squares solvers","title":"Investigate when the number of iterations is large","text":"","category":"section"},{"location":"experiments/ls_convergence/","page":"Convergence of least squares solvers","title":"Convergence of least squares solvers","text":"n = 9\nm = n\n\nr = reverse([i/n for i=1:n-1])\nforward = ForwardProblem(r)\nΛ(σ) = [forward_map(forward, j, σ) for j=1:m]\n\niσ_true = argmax(iter_tab_9)\nσ_true = σ_true_tab_9[:, iσ_true]\nσ_init = σ_init_tab_9[:, iσ_true]\nobs_true = Λ(σ_true)\n\nf(σ, p) = Λ(sqrt.(σ.^2 .+ offset)) .- obs_true\n\nproblem = NonlinearProblem(f, σ_init)\nres = NonlinearSolve.solve(\n    problem,\n    solver,\n    abstol=1e-15,\n    maxiters=200000,\n    store_trace=Val(true),\n    trace_level=TraceAll()\n)\n\nσ_hat = sqrt.((res.u).^2 .+ offset)\n\nobj(σ) = 0.5 * sum((Λ(σ) .- obs_true).^2)\ngrad_obj(σ) = gradient(obj, backend, σ)\n\nlength_trace = length(res.trace.history)\nσ_tab = [sqrt.((res.trace.history[i].u).^2 .+ offset) for i=1:length_trace]\nresidual_tab = [maximum(abs.(Λ(σ_tab[i]) .- obs_true)) for i=1:length_trace]\ndist_tab = [maximum(abs.(σ_tab[i] .- σ_true)) for i=1:length_trace]\ngrad_norm_tab = [maximum(abs.(grad_obj(σ_tab[i]))) for i=1:length_trace]\n\nplot(\n    residual_tab,\n    xscale=:log10,\n    yscale=:log10,\n    xlabel=L\"\\mathrm{iteration}\",\n    label=\"residual norm\",\n    c=:blue,\n    yticks = 10 .^ (-16.0:2.0:10.0),\n    xticks = 10 .^ (0:1:6),\n    legend=:bottomleft,\n    lw=2,\n    xtickfontsize=15,\n    ytickfontsize=15,\n    legendfontsize=12,\n    labelfontsize=16,\n    formatter=:latex\n)\n\nplot!(\n    dist_tab,\n    label=\"distance to solution\",\n    c=:red,\n    lw=2\n)\n\nplot!(\n    grad_norm_tab,\n    label=\"gradient norm\",\n    c=:green,\n    lw=2\n)","category":"page"},{"location":"experiments/ls_convergence/","page":"Convergence of least squares solvers","title":"Convergence of least squares solvers","text":"We see that, for the instance with the largest number of iterations, the iterates go very far away from the true conductivity while keeping a low value for the residual. After a very large number of iterations, the iterates finally converge to the true conductivity. This is likely due to the ill-posedness of the problem, which allows having very different conductivities while having similar measurements.","category":"page"},{"location":"experiments/ls_convergence/#Check-convergence-for-n10","page":"Convergence of least squares solvers","title":"Check convergence for n=10","text":"","category":"section"},{"location":"experiments/ls_convergence/","page":"Convergence of least squares solvers","title":"Convergence of least squares solvers","text":"n = 10\nm = n\n\nσ_true_tab = a .+ (b-a) .* rand(n, nσ_true)\nσ_init_tab = a .+ (b-a) .* rand(n, nσ_true)\n\nσ_hat_tab = zeros(n, nσ_true)\niter_tab = zeros(Int, nσ_true)\nerr_tab = zeros(nσ_true)\nerr_tab_measurements = zeros(nσ_true)\n\nr = reverse([i/n for i=1:n-1])\nforward = ForwardProblem(r)\nΛ(σ) = [forward_map(forward, j, σ) for j=1:m]\n\nmaxiters = 500000\n\nfor iσ_true=1:nσ_true\n    local σ_true = σ_true_tab[:, iσ_true]\n    local σ_init = σ_init_tab[:, iσ_true]\n    local obs_true = Λ(σ_true)\n\n    f(σ, p) = Λ(sqrt.(σ.^2 .+ offset)) .- obs_true\n\n    local problem = NonlinearProblem(f, σ_init)\n    local res = NonlinearSolve.solve(problem, solver, abstol=1e-15, maxiters=maxiters)\n    local σ_hat = sqrt.((res.u).^2 .+ offset)\n\n    σ_hat_tab[:, iσ_true] .= σ_hat\n    iter_tab[iσ_true] = res.stats.nsteps\n\n    err = abs.(σ_hat .- σ_true) ./ abs.(σ_true)\n    err_tab[iσ_true] = maximum(err)\n    err_tab_measurements[iσ_true] = maximum(abs.(Λ(σ_hat) .- obs_true))\nend\n\n@info \"number of convergence failures: $(sum(err_tab_measurements .> 1e-15)) out of $(nσ_true)\"\n\nfor iσ_true=1:nσ_true\n    if err_tab_measurements[iσ_true] > 1e-15\n        @info \"instance $iσ_true: $(iter_tab[iσ_true]) iterations (max $maxiters)\"\n    end\nend","category":"page"},{"location":"experiments/ls_convergence/","page":"Convergence of least squares solvers","title":"Convergence of least squares solvers","text":"The only instances for which the solver fails to converge have reached the maximum number of iterations.","category":"page"},{"location":"experiments/ls_convergence/","page":"Convergence of least squares solvers","title":"Convergence of least squares solvers","text":"","category":"page"},{"location":"experiments/ls_convergence/","page":"Convergence of least squares solvers","title":"Convergence of least squares solvers","text":"This page was generated using Literate.jl.","category":"page"},{"location":"experiments/ls_sdp/#Comparison-of-least-squares-and-nonlinear-SDP","page":"Least squares vs nonlinear SDP","title":"Comparison of least squares and nonlinear SDP","text":"","category":"section"},{"location":"experiments/ls_sdp/#Setting","page":"Least squares vs nonlinear SDP","title":"Setting","text":"","category":"section"},{"location":"experiments/ls_sdp/","page":"Least squares vs nonlinear SDP","title":"Least squares vs nonlinear SDP","text":"using RadialCalderon\n\nusing Optimization, OptimizationMOI\nusing JuMP\nimport Ipopt\n\nusing DifferentiationInterface\nimport ForwardDiff\n\nusing NonlinearSolve\n\nusing Plots\nusing LaTeXStrings\n\nusing Random\nusing Base.Iterators: product\n\nRandom.seed!(1234);  # fix the random seed for reproducibility\n\nn = 3\na = 0.5\nb = 1.5\nm = 7\n\nr = reverse([i/n for i=1:n-1])\n\nforward = ForwardProblem(r)\n\nΛ(σ) = [forward_map(forward, j, σ) for j=1:m]\ndΛ(σ) = jacobian(Λ, AutoForwardDiff(), σ);\nnothing #hide","category":"page"},{"location":"experiments/ls_sdp/#Estimation-of-the-weight-vector-c","page":"Least squares vs nonlinear SDP","title":"Estimation of the weight vector c","text":"","category":"section"},{"location":"experiments/ls_sdp/","page":"Least squares vs nonlinear SDP","title":"Least squares vs nonlinear SDP","text":"k = 5\n\nprod_it = product([range(a, b, k) for i=1:forward.n]...)\nσ = hcat(collect.(collect(prod_it))...)\nσ = σ[:, 1:end-1];  # remove b*ones(n)\n\nmodel = build_c_estimation_problem(σ, a, b, m, forward, max_last_coord=true)\noptimizer = optimizer_with_attributes(\n    Ipopt.Optimizer,\n    \"print_level\" => 0,\n    \"sb\" => \"yes\",\n    \"tol\" => 1e-13,\n    \"constr_viol_tol\" => 1e-15\n)\nset_optimizer(model, optimizer)\noptimize!(model)\nc = value.(model[:c])\n\n@info \"c estimation problem solved: $(is_solved_and_feasible(model))\"\n@info \"vector c: $c\"","category":"page"},{"location":"experiments/ls_sdp/#Reconstruction-via-least-squares-and-nonlinear-SDP","page":"Least squares vs nonlinear SDP","title":"Reconstruction via least squares and nonlinear SDP","text":"","category":"section"},{"location":"experiments/ls_sdp/","page":"Least squares vs nonlinear SDP","title":"Least squares vs nonlinear SDP","text":"nσ_test = 100\nσ_test = a .+ (b-a) .* rand(n, nσ_test)\nσ_init_sdp = (0.9*b) .* ones(n)\n\nσ_hat_tab_sdp = zeros(n, nσ_test)\nσ_hat_tab_ls = zeros(n, nσ_test)\n\noptimizer = OptimizationMOI.MOI.OptimizerWithAttributes(\n    Ipopt.Optimizer,\n    \"print_level\" => 0,\n    \"sb\" => \"yes\",\n    \"tol\" => 1e-15,\n    \"constr_viol_tol\" => 1e-15\n)\n\nfor iσ_test=1:nσ_test\n    σ_true = σ_test[:, iσ_test]\n    obs_true = Λ(σ_true)\n\n    convex_calderon = ConvexCalderonProblem(c, a, b, obs_true, forward)\n    sdp = build_nonlinear_sdp(convex_calderon, σ_init_sdp)\n    sol_sdp = solve(sdp, optimizer)\n    σ_hat_sdp = sol_sdp.u\n    σ_hat_tab_sdp[:, iσ_test] .= σ_hat_sdp\n\n    f(σ, p) = Λ(σ) .- obs_true\n    σ_init_ls = a .+ (b-a) .* rand(n)\n    problem = NonlinearProblem(f, σ_init_ls)\n    sol_ls = NonlinearSolve.solve(problem, RobustMultiNewton(), abstol=1e-15)\n    σ_hat_ls = sol_ls.u\n    σ_hat_tab_ls[:, iσ_test] .= σ_hat_ls\nend","category":"page"},{"location":"experiments/ls_sdp/","page":"Least squares vs nonlinear SDP","title":"Least squares vs nonlinear SDP","text":"We can now plot the histogram of the estimation errors for both methods.","category":"page"},{"location":"experiments/ls_sdp/","page":"Least squares vs nonlinear SDP","title":"Least squares vs nonlinear SDP","text":"linf_err_tab_sdp = vcat(maximum(abs.(σ_hat_tab_sdp .- σ_test), dims=1)...)\nlinf_err_tab_ls = vcat(maximum(abs.(σ_hat_tab_ls .- σ_test), dims=1)...)\n\nbin = 10.0 .^ (range(-17, -7, 20))\nhistogram(\n    [linf_err_tab_ls, linf_err_tab_sdp],\n    bin=bin,\n    xscale=:log10,\n    xlim=extrema(bin),\n    ylim=(0, 50),\n    xticks=10.0 .^ [-15, -13, -11, -9],\n    fillalpha=0.3,\n    fillcolor=[:blue :green],\n    label=[\"Newton\" \"SDP\"],\n    xlabel=\"Estimation error\",\n    formatter=:latex,\n    xtickfontsize=20,\n    ytickfontsize=20,\n    legendfontsize=18,\n    labelfontsize=21,\n    bottom_margin=5Plots.mm\n)","category":"page"},{"location":"experiments/ls_sdp/","page":"Least squares vs nonlinear SDP","title":"Least squares vs nonlinear SDP","text":"","category":"page"},{"location":"experiments/ls_sdp/","page":"Least squares vs nonlinear SDP","title":"Least squares vs nonlinear SDP","text":"This page was generated using Literate.jl.","category":"page"},{"location":"ref/#References","page":"References","title":"References","text":"","category":"section"},{"location":"ref/","page":"References","title":"References","text":"Harrach, B. (2023). The Calderón Problem with Finitely Many Unknowns Is Equivalent to Convex Semidefinite Optimization. SIAM Journal on Mathematical Analysis, 5666–5684.\n\n\n\n","category":"page"},{"location":"#RadialCalderon.jl","page":"Home","title":"RadialCalderon.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"(Image: Dev) (Image: Aqua QA) (Image: Build Status)","category":"page"},{"location":"","page":"Home","title":"Home","text":"A Julia package for studying the Calderon problem with piecewise constant radial conductivities.","category":"page"},{"location":"#Getting-started","page":"Home","title":"Getting started","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To install this package, run the following command from the REPL in Pkg mode.","category":"page"},{"location":"","page":"Home","title":"Home","text":"pkg> add https://github.com/rpetit/RadialCalderon.jl","category":"page"},{"location":"","page":"Home","title":"Home","text":"Tutorials and examples are available in the documentation.","category":"page"},{"location":"#Citation","page":"Home","title":"Citation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you use this package in your research, please cite the following preprint.","category":"page"},{"location":"","page":"Home","title":"Home","text":"@misc{alberti2025nonconvexitycalderon,\n    title={On the non-convexity issue in the radial Calder\\'on problem}, \n    author={Giovanni S. Alberti and Romain Petit and Clarice Poon},\n    year={2025},\n    eprint={2507.03379},\n    archivePrefix={arXiv},\n    primaryClass={math.NA},\n    url={https://arxiv.org/abs/2507.03379}, \n}","category":"page"},{"location":"examples/least_squares/#Reconstruction-via-nonlinear-least-squares","page":"Least squares","title":"Reconstruction via nonlinear least squares","text":"","category":"section"},{"location":"examples/least_squares/","page":"Least squares","title":"Least squares","text":"In this tutorial, we show how to implement the least squares approach to solve the Calderón problem in the noiseless setting. To be more precise, we wish to minimize the functional","category":"page"},{"location":"examples/least_squares/","page":"Least squares","title":"Least squares","text":"fsigmamapsto frac12Lambda(sigma)-Lambda(sigma^dagger)_2^2","category":"page"},{"location":"examples/least_squares/","page":"Least squares","title":"Least squares","text":"where sigma^dagger is an unknown conductivity. Since Lambda is nonlinear, f is non-convex and iterative minimization algorithms could in principle suffer from the problem of local convergence (their output might heavily depend on their initialization). However, we will see that, in practice, the main issue is rather the ill-posedness of the inverse problem, and that robust iterative algorithms almost always converge to a global minmizer regardless of their initialization.","category":"page"},{"location":"examples/least_squares/#Setting","page":"Least squares","title":"Setting","text":"","category":"section"},{"location":"examples/least_squares/","page":"Least squares","title":"Least squares","text":"using RadialCalderon\n\nn = 3\nr = [0.5, 0.25]\nforward = ForwardProblem(r)\n\nm = n\nΛ(σ) = [forward_map(forward, j, σ) for j in 1:m]  # forward map\n\na = 0.5\nb = 1.5\n\nσ_true = [0.8, 1.2, 1.0]  # unknown conductivity\nobs_true = Λ(σ_true);  # observations\nnothing #hide","category":"page"},{"location":"examples/least_squares/#Using-[NonlinearSolve.jl](https://github.com/SciML/NonlinearSolve.jl)","page":"Least squares","title":"Using NonlinearSolve.jl","text":"","category":"section"},{"location":"examples/least_squares/","page":"Least squares","title":"Least squares","text":"We use the package NonlinearSolve.jl to solve the equation Lambda(sigma)=Lambda(sigma^dagger) using a robust Newton-type algorithm initialized with a guess sigma_mathrminit. We notice that, in practice, the method converges to a solution regardless of its initialization.","category":"page"},{"location":"examples/least_squares/","page":"Least squares","title":"Least squares","text":"using Base.Iterators: product\n\nk = 5\nprod_it = product([range(a, b, k) for i=1:3]...)\nσ_init_tab = hcat(collect.(collect(prod_it))...)\nnσ_init = size(σ_init_tab, 2)\n\n@info \"Number of initializations: $nσ_init\"","category":"page"},{"location":"examples/least_squares/","page":"Least squares","title":"Least squares","text":"The set of initial guesses is a regular discretization of ab^n using k^n points (here, we chose k=5).","category":"page"},{"location":"examples/least_squares/","page":"Least squares","title":"Least squares","text":"using NonlinearSolve\n\nresidual(σ, p) = Λ(σ) .- obs_true\nmax_linf_err = 0.0\nmax_linf_err_obs = 0.0\n\nfor i=1:nσ_init\n    σ_init = σ_init_tab[:, i]\n    problem = NonlinearProblem(residual, σ_init)\n    res = NonlinearSolve.solve(problem, RobustMultiNewton(), abstol=1e-15)\n    σ_hat = res.u\n    global max_linf_err = max(max_linf_err, maximum(abs.(σ_hat .- σ_true)))\n    global max_linf_err_obs = max(max_linf_err_obs, maximum(abs.(Λ(σ_hat) .- obs_true)))\nend\n\n@info \"Maximum l-infinity error (conductivity): $max_linf_err\"\n@info \"Maximum l-infinity error (observations): $max_linf_err_obs\"","category":"page"},{"location":"examples/least_squares/#Using-[Optimization.jl](https://github.com/SciML/Optimization.jl)","page":"Least squares","title":"Using Optimization.jl","text":"","category":"section"},{"location":"examples/least_squares/","page":"Least squares","title":"Least squares","text":"We can also use the Optimization.jl package to call a large list of optimization algorithms to minimize f. Here, we use an interior point Newton algorithm from the Optim.jl package. We enforce the constraint sigma_i10^-5 to avoid domain errors.","category":"page"},{"location":"examples/least_squares/","page":"Least squares","title":"Least squares","text":"using Optimization\nusing OptimizationOptimJL\nusing DifferentiationInterface\nimport ForwardDiff\n\nobj(σ, p) = 0.5 * sum((Λ(σ) .- obs_true).^2)\noptfun = OptimizationFunction(obj, SecondOrder(AutoForwardDiff(), AutoForwardDiff()))\n\nmax_linf_err = 0.0\nmax_linf_err_obs = 0.0\n\nfor i=1:nσ_init\n    σ_init = σ_init_tab[:, i]\n    problem = OptimizationProblem(optfun, σ_init, lb=1e-5.*ones(n), ub=Inf.*ones(n))\n    res = solve(problem, IPNewton(), g_tol=1e-17)\n    σ_hat = res.u\n    global max_linf_err = max(max_linf_err, maximum(abs.(σ_hat .- σ_true)))\n    global max_linf_err_obs = max(max_linf_err_obs, maximum(abs.(Λ(σ_hat) .- obs_true)))\nend\n\n@info \"Maximum l-infinity error (conductivity): $max_linf_err\"\n@info \"Maximum l-infinity error (observations): $max_linf_err_obs\"","category":"page"},{"location":"examples/least_squares/","page":"Least squares","title":"Least squares","text":"","category":"page"},{"location":"examples/least_squares/","page":"Least squares","title":"Least squares","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/nonlinear_sdp/#Reconstruction-via-convex-programming","page":"Convex nonlinear SDP","title":"Reconstruction via convex programming","text":"","category":"section"},{"location":"examples/nonlinear_sdp/","page":"Convex nonlinear SDP","title":"Convex nonlinear SDP","text":"In this tutorial, we show how to implement in practice the reconstruction method introduced in (Harrach, 2023). It is based on the resolution of a convex nonlinear semidefinite program of the form","category":"page"},{"location":"examples/nonlinear_sdp/","page":"Convex nonlinear SDP","title":"Convex nonlinear SDP","text":"undersetsigmainab^nmathrmminlangle csigmaranglemathrmstLambda(sigma)leq y","category":"page"},{"location":"examples/nonlinear_sdp/","page":"Convex nonlinear SDP","title":"Convex nonlinear SDP","text":"where c is a weight vector with positive entries and y is the vector of observations. If the number of measurements m is large enough, then there exists a vector c such that, for every sigma^daggerinab^n, the above problem with y=Lambda(sigma^dagger) has a unique solution which is sigma^dagger.","category":"page"},{"location":"examples/nonlinear_sdp/#Setting","page":"Convex nonlinear SDP","title":"Setting","text":"","category":"section"},{"location":"examples/nonlinear_sdp/","page":"Convex nonlinear SDP","title":"Convex nonlinear SDP","text":"We define the forward problem. There are two annuli with the inner radius being 0.5.","category":"page"},{"location":"examples/nonlinear_sdp/","page":"Convex nonlinear SDP","title":"Convex nonlinear SDP","text":"using RadialCalderon\n\nn = 2\nr = [0.5]\nforward = ForwardProblem(r)\n\na = 0.5\nb = 1.5;\nnothing #hide","category":"page"},{"location":"examples/nonlinear_sdp/#Estimation-of-the-weight-vector","page":"Convex nonlinear SDP","title":"Estimation of the weight vector","text":"","category":"section"},{"location":"examples/nonlinear_sdp/","page":"Convex nonlinear SDP","title":"Convex nonlinear SDP","text":"For a given m, one can try to find a universal vector c by solving a feasibility problem constructed via build_c_estimation_problem. Below, we check that, when n=2, this is possible for m=3 but not for m=2. First, we define a set of conductivities which will be used to estimate c.","category":"page"},{"location":"examples/nonlinear_sdp/","page":"Convex nonlinear SDP","title":"Convex nonlinear SDP","text":"using Base.Iterators: product\n\nk = 2\n\nprod_it = product([range(a, b, k) for i=1:forward.n]...)\nσ = hcat(collect.(collect(prod_it))...)\nσ = σ[:, 1:end-1];  # remove b*ones(n)\nnothing #hide","category":"page"},{"location":"examples/nonlinear_sdp/","page":"Convex nonlinear SDP","title":"Convex nonlinear SDP","text":"We build the c estimation problem for m=2 and check that it is not feasible. The problem is a linear program, so that any suitable solver other than Ipopt can be used (in our experiments, MOSEK performed best).","category":"page"},{"location":"examples/nonlinear_sdp/","page":"Convex nonlinear SDP","title":"Convex nonlinear SDP","text":"using JuMP\nimport Ipopt\n\nm = 2\nmodel = build_c_estimation_problem(σ, a, b, m, forward)\noptimizer = optimizer_with_attributes(Ipopt.Optimizer, \"print_level\" => 0, \"sb\" => \"yes\")\nset_optimizer(model, optimizer)\noptimize!(model)","category":"page"},{"location":"examples/nonlinear_sdp/","page":"Convex nonlinear SDP","title":"Convex nonlinear SDP","text":"We check that there is no admissible vector c for m=2.","category":"page"},{"location":"examples/nonlinear_sdp/","page":"Convex nonlinear SDP","title":"Convex nonlinear SDP","text":"is_solved_and_feasible(model)","category":"page"},{"location":"examples/nonlinear_sdp/","page":"Convex nonlinear SDP","title":"Convex nonlinear SDP","text":"We check that there is an admissible vector c for m=3.","category":"page"},{"location":"examples/nonlinear_sdp/","page":"Convex nonlinear SDP","title":"Convex nonlinear SDP","text":"m = 3\nmodel = build_c_estimation_problem(σ, a, b, m, forward)\nset_optimizer(model, optimizer)\noptimize!(model)\nis_solved_and_feasible(model)","category":"page"},{"location":"examples/nonlinear_sdp/","page":"Convex nonlinear SDP","title":"Convex nonlinear SDP","text":"The estimated vector c can be accessed as follows.","category":"page"},{"location":"examples/nonlinear_sdp/","page":"Convex nonlinear SDP","title":"Convex nonlinear SDP","text":"c = value.(model[:c])","category":"page"},{"location":"examples/nonlinear_sdp/","page":"Convex nonlinear SDP","title":"Convex nonlinear SDP","text":"We stress that, when n is larger, the tolerance of the solver might have to be adjusted to ensure that the problem is solved with good precision.","category":"page"},{"location":"examples/nonlinear_sdp/#Resolution-of-the-convex-nonlinear-SDP","page":"Convex nonlinear SDP","title":"Resolution of the convex nonlinear SDP","text":"","category":"section"},{"location":"examples/nonlinear_sdp/","page":"Convex nonlinear SDP","title":"Convex nonlinear SDP","text":"Once the weight vector c is estimated, one can solve the convex nonlinear SDP defined above. To do so, we build the problem using build_nonlinear_sdp and use the Optimization package with the Ipopt solver.","category":"page"},{"location":"examples/nonlinear_sdp/","page":"Convex nonlinear SDP","title":"Convex nonlinear SDP","text":"using Optimization, OptimizationMOI\n\nσ_true = [0.8, 1.2]  # unknown conductivity\nobs = [forward_map(forward, j, σ_true) for j=1:m]  # observations\nproblem = ConvexCalderonProblem(c, a, b, obs, forward)\n\nσ_init = (0.9*b) .* ones(forward.n)  # intial guess\nprob = build_nonlinear_sdp(problem, σ_init)  # container for the nonlinear SDP\n\noptimizer = OptimizationMOI.MOI.OptimizerWithAttributes(\n    Ipopt.Optimizer,\n    \"print_level\" => 0,\n    \"sb\" => \"yes\"\n)\n\nsol = solve(prob, optimizer)\nσ_hat = sol.u\n\nisapprox(σ_hat, σ_true, rtol=1e-6)","category":"page"},{"location":"examples/nonlinear_sdp/","page":"Convex nonlinear SDP","title":"Convex nonlinear SDP","text":"","category":"page"},{"location":"examples/nonlinear_sdp/","page":"Convex nonlinear SDP","title":"Convex nonlinear SDP","text":"This page was generated using Literate.jl.","category":"page"}]
}
